<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="On the Effective Horizon of Inverse Reinforcement Learning">
  <meta name="keywords" content="Inverse Reinforcement Learning; Imitation Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>On the Effective Horizon of Inverse Reinforcement Learning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://arxiv.org/abs/2307.06541">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">On the Effective Horizon of Inverse Reinforcement Learning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Yiqing Xu<sup>1</sup>,
            </span>
            <span class="author-block">
              Finale Doshi-Velez<sup>2</sup>,
            </span>
            <span class="author-block">
              David Hsu<sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>School of Computing,
              Smart System Institute, National University of Singapore</span>
            <span class="author-block"><sup>2</sup>Department of Computer Science, Harvard University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2307.06541"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2307.06541"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>ArXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/eeching/Effective_Horizon_IRL"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered">
        <div class="column is-full">
          <img id="teaserGif" src="./static/images/ehril_hook.gif" alt="Teaser GIF" style="width: 100%; height: auto;">
          <figcaption class="subtitle has-text-centered">We investigate why, in Inverse Reinforcement Learning, an effective time horizon shorter than the ground-truth value often produces better results faster.</figcaption>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Inverse reinforcement learning (IRL) algorithms often rely on (forward) reinforcement learning or planning over a given time horizon to compute an approximately optimal policy for a hypothesized reward function and then match this policy with expert demonstrations. The time horizon plays a critical role in determining both the accuracy of reward estimates and the computational efficiency of IRL algorithms. Interestingly, an <em>effective time horizon</em> shorter than the ground-truth value often produces better results faster. This work formally analyzes this phenomenon and provides an explanation: the time horizon controls the complexity of an induced policy class and mitigates overfitting with limited data. This analysis serves as a guide for the principled choice of the effective horizon for IRL. It also prompts us to re-examine the classic IRL formulation: it is more natural to learn jointly the reward and the effective horizon  rather than the reward alone with a given horizon. To  validate our findings, we implement a cross-validation extension and the experimental results confirm the theoretical analysis.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Theortical Insight</h2>   
        <div class="content has-text-justified">
          <object data="./static/images/loss_decomposition.gif" type="image/gif" width="100%">
            <!-- Fallback content for browsers that don't support the object tag -->
            <img src="./static/images/loss_decomposition.gif" alt="Loss Decomposition">
          </object>
          <p>We present a formal analysis showing that, with limited expert data, using a shorter discount factor or time horizon improves the generalization of the learned reward function to unseen states. In IRL settings where the effective horizon varies, the performance gap between the induced policy and the expert policy arises from two sources: (i) reward estimation error due to limited data during IRL, and (ii) policy optimization error from using an effective horizon shorter than the ground-truth. We prove that the effective horizon controls the complexity of the approximated policy class during IRL. As the horizon increases, reward estimation error grows‚Äîoverfitting occurs because we estimate policies from a more complex class using limited expert data. On the other hand, as the horizon approaches the ground-truth, the policy optimization error decreases. These opposing errors suggest that an intermediate effective horizon balances this trade-off and produces the most expert-like policy.</p>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experimental Validation</h2>
        <p>We empirically examine our theoretical findings by exploring how discount factors influence IRL performance with varying amounts of expert data. We adapt Linear Programming IRL (LP-IRL) and Maximum Entropy IRL (MaxEnt-IRL) to accommodate different discount factor settings and expert data sizes. Specifically, We evaluate the performance of LP-IRL and MaxEnt-IRL on four Gridworld and Objectworld tasks with varying reward complexity. To jointly learn the reward-horizon pair, we incorporate cross-validation into our modified IRL methods to optimize the discount factor. By evaluating policies across a range of discount factors/horizons, cross-validation allows us to directly observe how varying bùõæ affects IRL performance with different amounts of expert data.</p>
        
        <div class="columns is-centered">
          <div class="column is-full">
            <object data="./static/images/lp_env_summary_Gridworld-simple.jpg" type="image/jpeg" width="100%" >
              <!-- Fallback content for browsers that don't support the object tag -->
              <img src="./static/images/lp_env_summary_Gridworld-simple.jpg" alt="lp_env_summary_Gridworld-simple">
            </object>
            <object data="./static/images/lp_env_summary_Gridworld-hard.jpg" type="image/jpeg" width="100%" >
              <!-- Fallback content for browsers that don't support the object tag -->
              <img src="./static/images/lp_env_summary_Gridworld-hard.jpg" alt="lp_env_summary_Gridworld-hard">
            </object>
            <object data="./static/images/lp_env_summary_Objectworld-linear.jpg" type="image/jpeg" width="100%" >
              <!-- Fallback content for browsers that don't support the object tag -->
              <img src="./static/images/lp_env_summary_Objectworld-linear.jpg" alt="lp_env_summary_Objectworld-linear">
            </object>
            <object data="./static/images/lp_env_summary_Objectworld-non_linear.jpg" type="image/jpeg" width="100%" >
              <!-- Fallback content for browsers that don't support the object tag -->
              <img src="./static/images/lp_env_summary_Objectworld-non_linear.jpg" alt="lp_env_summary_Objectworld-non_linear">
            </object>
            <figcaption class="has-text-centered">Summary of LP-IRL with varying discount factors across four tasks.</figcaption>
          </div>
          <p>The <em>error counts</em> measure the number of states for which a policy's action selection deviates from the expert's actions. Each task displays the ground-truth value function (column 1), reward function (column 2), expert policy (column 3), error count curves for different amount of expert data in a <strong>single instance</strong> (columns 4-8), and the error count curve summary for a <strong>batch</strong> of 10 MDPs across varying amount of expert data (column 9). In all four tasks, ground-truth discount factor is 0.99. The optimal discount factor is smaller than the ground-truth value for varying amount of expert data.</p>
        </div>

        <div class="columns is-centered">
          <div class="column is-full">
            <object data="./static/images/lp_coverage_vs_gammas.jpg" type="image/jpeg" width="100%" >
              <!-- Fallback content for browsers that don't support the object tag -->
              <img src="./static/images/lp_coverage_vs_gammas.jpg" alt="lp_coverage_vs_gammas">
            </object>
            <figcaption class="has-text-centered">Optimal discount factors for LP-IRL at varying amount of expert data. </figcaption>
          </div>
          <p>For each task, we select the optimal discount factors for all 10 sampled environments through cross-validation. The orange curves illustrate how the optimal discount factor changes with the amount of expert data, while the green curves show the corresponding error counts. The ground-truth value (0.99) is depicted in grey, with its error counts displayed in blue. As the amount of expert data increases, the optimal discount factor initially decreases sharply and then gradually increases, indicating that overfitting is prominent when expert data is scarce.</p>  
        </div>

        <div class="columns is-centered">
          <div class="column is-full">
            <object data="./static/images/maxent_env_summary_Gridworld-simple.jpg" type="image/jpeg" width="100%" >
              <!-- Fallback content for browsers that don't support the object tag -->
              <img src="./static/images/maxent_env_summary_Gridworld-simple.jpg" alt="maxent_env_summary_Gridworld-simple">
            </object>
            <object data="./static/images/maxent_env_summary_Gridworld-hard.jpg" type="image/jpeg" width="100%" >
              <!-- Fallback content for browsers that don't support the object tag -->
              <img src="./static/images/maxent_env_summary_Gridworld-hard.jpg" alt="maxent_env_summary_Gridworld-hard">
            </object>
            <object data="./static/images/maxent_env_summary_Objectworld-linear.jpg" type="image/jpeg" width="100%" >
              <!-- Fallback content for browsers that don't support the object tag -->
              <img src="./static/images/maxent_env_summary_Objectworld-linear.jpg" alt="maxent_env_summary_Objectworld-linear">
            </object>
            <object data="./static/images/maxent_env_summary_Objectworld-non_linear.jpg" type="image/jpeg" width="100%" >
              <!-- Fallback content for browsers that don't support the object tag -->
              <img src="./static/images/maxent_env_summary_Objectworld-non_linear.jpg" alt="maxent_env_summary_Objectworld-non_linear">
            </object>
            <figcaption class="has-text-centered">Summary of MaxEnt-IRL with different horizons for the four tasks. </figcaption>
          </div>
          <p>For each task, we present the ground-truth value function (column 1), ground-truth reward function (column 2), expert policy (column 3), error count curves in all states for different amount of expert data for a <strong>single instance</strong> of the task (columns 4-8), and finally, a summary of the error curves for a <strong>batch</strong> of 10 MDPs (column 9). In all four tasks, the ground-truth horizon is 20. The optimal horizon is smaller than the ground-truth value for varying amount of expert data.</p>
        </div>

        <div class="columns is-centered">
          <div class="column is-full">
            <object data="./static/images/maxent_coverage_vs_horizons.jpg" type="image/jpeg" width="100%" >
              <!-- Fallback content for browsers that don't support the object tag -->
              <img src="./static/images/maxent_coverage_vs_horizons.jpg" alt="maxent_coverage_vs_horizons">
            </object>
            <figcaption class="has-text-centered">Optimal horizons for MaxEnt-IRL at varying amount of expert data.</figcaption>
          </div>
          <p> We select the optimal horizon values for each of 10 sampled task environments using cross-validation, based on the amount of expert data. Orange curves show how optimal horizon value changes with the amount of expert data, while green curves display corresponding error counts. The ground-truth horizon (20) is depicted by a grey line, with corresponding error lines in blue. The trends are consistent with LP-IRL.</p>
        </div>
      </div>
    </div>

</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @inproceedings{xu2025effective,
        title={On the Effective Horizon of Inverse Reinforcement Learning},
        author={Xu, Yiqing and Doshi-Velez, Finale and Hsu, David},
        booktitle={International Conference on Autonomous Agents and Multiagent Systems},
        year={2025}
      }
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/abs/2405.11928">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
